---
title: Understanding the Process of Reading
author: Benjamin T. Carter
date: 2019-10-24
output:
  html_document:
    code_folding: hide
---

## Environment

```{r message = FALSE}
library(dplyr)
library(readxl)
library(tidyr)
```

## Cleaning statistics

These are some shortcuts that can be used to view the cleaning stats via command line. I've commented them out since they only need to be run once.

```{bash}
# DIRECT=~/Box/LukeLab/UnderstandingTheProcessOfReading/data
# DATA=${DIRECT}/cleaning\ -\ UPoR1.txt # file with cleaning stats

# mv $DATA ${DIRECT}/cleaning-UPoR1.txt
# DATA=${DIRECT}/cleaning-UPoR1.txt

# tail -2 ${DATA}/cleaning-UPoR1.txt # read in last two lines
```

The stats were then taken to compute how many fix

```{r}
MERGED <- 738
DELETED <- 9985
DIR <- file.path("~","Box","LukeLab","UnderstandingTheProcessOfReading","data") # study directory
FIXS <- read.table(file.path(DIR,"FixList.xls"), header = TRUE, sep = "\t") # list of fixations
TOTAL <- as.numeric(count(FIXS)) # count total number of fixation events
REMAIN <- (MERGED + DELETED) / TOTAL
REMAIN*100 # percent of fixations that were excluded or merged.
```

## Behavior

### Data Cleaning and Aggregation

Here are the paths to the data. Let's bring them together along common variables.

```{r}
# read in the data
DIR <- file.path("~","Box","LukeLab","UnderstandingTheProcessOfReading","data") # study directory
REPORT <- read.delim(file.path(DIR,"IA_report_UPoR1.txt"), header = TRUE, sep = "\t", fill = TRUE, na.strings = ".") # interest areas report
ORTHOS <- read.csv(file.path(DIR,"Provo_Corpus-Eyetracking_Data.csv"), header = TRUE, sep = ",", fill = TRUE) # ortho probabilities

# clean the interest areas report
REPORT$RECORDING_SESSION_LABEL <- tolower(REPORT$RECORDING_SESSION_LABEL) # make label case consistent

exclude <- c("s32", "s46", "s53", "s54", "s56", "s57", "s59", "s60", "s63") # exclude bad data
REPORT <- REPORT %>% subset(!(RECORDING_SESSION_LABEL %in% exclude))


# aggregate the Provo Corpus and join to the interest areas report
COMBINED <- ORTHOS[c("Text_ID","IA_ID","OrthoMatchModel")] %>% 
  group_by(Text_ID, IA_ID) %>%
  summarize(mean_OrthoMatchModel = mean(OrthoMatchModel)) %>%
  ungroup() %>%
  right_join(REPORT,
             by = c("Text_ID" = "textnumber", "IA_ID" = "IA_ID")
             )

COMBINED <- COMBINED[!is.na(COMBINED$mean_OrthoMatchModel),] # remove na values
rm(ORTHOS, REPORT) # unload unneeded data

# need to add demographic data to this too!

# change classes
COMBINED$windowcondition <- as.factor(COMBINED$windowcondition)
COMBINED$Text_ID <- as.factor(COMBINED$Text_ID)
COMBINED$IA_ID <- as.factor(COMBINED$IA_ID)
levels(COMBINED$windowcondition) <- list("No Preview"="0", "Preview" = "1")

# add variable for how far into a word someone fixated
COMBINED <- COMBINED %>%
  mutate(
    FIRST_LANDING = (IA_FIRST_FIXATION_X - IA_LEFT) / (IA_RIGHT - IA_LEFT)
  )

# create skipping probability
COMBINED <- COMBINED %>%
  mutate(
    SKIP_PROB = 
  )

# create refixation probability

# create regression probability

COMBINED$WORD_ID <- interaction(COMBINED$Text_ID, COMBINED$IA_ID, sep = ".") # create interaction variable

# check out https://link.springer.com/article/10.3758/s13414-018-1581-0
# log transform dependent variables, add additional variables from ^paper^
```

### Summary Statistics by Window Condition
```{r}
vars <- c("windowcondition", "IA_FIRST_FIXATION_DURATION", "FIRST_LANDING", "IA_DWELL_TIME") # need to add skipping probability, single fixation duration, refixation probability, regression probability, total time, gaze duration.
summaryStats <- COMBINED %>%
  select(vars) %>%
  group_by(windowcondition) %>%
  summarize(
    "First Fixation Duration - mean" = mean(IA_FIRST_FIXATION_DURATION),
    "First Fixation Duration - sd" = sd(IA_FIRST_FIXATION_DURATION),
    "First Fixation Location - mean" = mean(FIRST_LANDING),
    "First Fixation Location - sd" = sd(FIRST_LANDING),
    "Dwell Time - mean" = mean(IA_DWELL_TIME),
    "Dwell Time - sd" = sd(IA_DWELL_TIME),
    "Skipping probability" = formula
    ) %>%
  gather(Statistic, Value, "First Fixation Duration - mean":"Landing Spot - sd") %>%
  spread(windowcondition, Value)

```

### Visualizing trends
```{r echo = FALSE, message=FALSE, error=FALSE}
ggplot(COMBINED, 
       aes(x = log(mean_OrthoMatchModel), y = IA_FIRST_FIXATION_DURATION, fill = windowcondition)) +
  geom_smooth(method = "lm") +
  labs(
    title = "The Effect of Preview and Frequency on First Fixation Duration ",
    x = "Predictability",
    y = "First Fixation Duration (ms)",
    fill = "Window Condition"
  )

ggplot(COMBINED, 
       aes(x = log(mean_OrthoMatchModel), y = IA_DWELL_TIME, fill = windowcondition)) +
  geom_smooth(method = "lm") +
  labs(
    title = "The Effect of Preview and Frequency on Dwell Time",
    x = "Predictability",
    y = "Total Dwell Time (ms)",
    fill = "Window Condition"
  )

ggplot(COMBINED, 
       aes(x = log(mean_OrthoMatchModel), y = FIRST_LANDING, fill = windowcondition)) +
  geom_smooth(method = "lm") +
  labs(
    title = "The Effect of Preview and Frequency on First Fixation Location",
    x = "Predictability",
    y = "First Fixation Location",
    fill = "Window Condition"
  )

```

### Models for First Fixation Duration

For the models below I excluded all interest areas that did not have a fixation.

```{r echo = FALSE, message=FALSE, error=FALSE}
# load libraries
library(lme4)
library(lmerTest)

# models
  # first fixation duration - failed to converge
mod <- lmer(
  log(IA_FIRST_FIXATION_DURATION) ~ windowcondition * scale(log(mean_OrthoMatchModel), scale = FALSE) +
    (1 + windowcondition * scale(log(mean_OrthoMatchModel), scale = FALSE)|RECORDING_SESSION_LABEL) +
    (1|WORD_ID),
  data = COMBINED %>% filter(IA_DWELL_TIME > 0)
)

  # second attempt - also failed to converge
mod <- lmer(
  log(IA_FIRST_FIXATION_DURATION) ~ windowcondition + scale(log(mean_OrthoMatchModel), scale = FALSE) +
    (1 + windowcondition + scale(log(mean_OrthoMatchModel), scale = FALSE)|RECORDING_SESSION_LABEL) +
    (1|WORD_ID),
  data = COMBINED %>% filter(IA_DWELL_TIME > 0)
)

  # third attempt - successful
mod1 <- lmer(
  log(IA_FIRST_FIXATION_DURATION) ~ windowcondition * scale(log(mean_OrthoMatchModel), scale = FALSE) +
    (1 + windowcondition|RECORDING_SESSION_LABEL) +
    (1|WORD_ID),
  data = COMBINED %>% filter(IA_DWELL_TIME > 0)
)
summary(mod1)

  # fourth model - failed to converge
mod <- lmer(
  log(IA_FIRST_FIXATION_DURATION) ~ windowcondition * scale(log(mean_OrthoMatchModel), scale = FALSE) +
    (1 + scale(log(mean_OrthoMatchModel), scale = FALSE)|RECORDING_SESSION_LABEL) +
    (1|WORD_ID),
  data = COMBINED %>% filter(IA_DWELL_TIME > 0)
)

  # fifth model - successful
mod2 <- lmer(
  log(IA_FIRST_FIXATION_DURATION) ~ windowcondition * scale(log(mean_OrthoMatchModel), scale = FALSE) +
    (1|RECORDING_SESSION_LABEL) +
    (1|WORD_ID),
  data = COMBINED %>% filter(IA_DWELL_TIME > 0)
)
summary(mod2)
```

### Models for Total Dwell Time
```{r echo = FALSE, message=FALSE, error=FALSE}
  # first model - failed to converge
mod <- lmer(
  log(IA_DWELL_TIME) ~ windowcondition * scale(log(mean_OrthoMatchModel), scale = FALSE) +
    (1 + windowcondition * scale(log(mean_OrthoMatchModel), scale = FALSE)|RECORDING_SESSION_LABEL) +
    (1|WORD_ID),
  data = COMBINED
)

  # second model - failed
mod <- lmer(
  log(IA_DWELL_TIME) ~ windowcondition + scale(log(mean_OrthoMatchModel), scale = FALSE) +
    (1 + windowcondition + scale(log(mean_OrthoMatchModel), scale = FALSE)|RECORDING_SESSION_LABEL) +
    (1|WORD_ID),
  data = COMBINED
)

  # third model
mod3 <- lmer(
  log(IA_DWELL_TIME) ~ windowcondition * scale(log(mean_OrthoMatchModel), scale = FALSE) +
    (1 + windowcondition|RECORDING_SESSION_LABEL) +
    (1|WORD_ID),
  data = COMBINED
)
summary(mod3)


```

### Models for First Fixation Position
```{r echo = FALSE,echo=FALSE,message=FALSE}
  # landing zone 1 - failed
mod <- lmer(
  FIRST_LANDING ~ windowcondition * scale(log(mean_OrthoMatchModel), scale = FALSE) +
    (1 + windowcondition * scale(log(mean_OrthoMatchModel), scale = FALSE)|RECORDING_SESSION_LABEL) +
    (1|WORD_ID),
  data = COMBINED
)

  # landing zone 2 - failed
mod <- lmer(
  FIRST_LANDING ~ windowcondition + scale(log(mean_OrthoMatchModel), scale = FALSE) +
    (1 + windowcondition + scale(log(mean_OrthoMatchModel), scale = FALSE)|RECORDING_SESSION_LABEL) +
    (1|WORD_ID),
  data = COMBINED
)

  # landing zone 3 - failed
mod <- lmer(
  FIRST_LANDING ~ windowcondition * scale(log(mean_OrthoMatchModel), scale = FALSE) +
    (1 + windowcondition|RECORDING_SESSION_LABEL) +
    (1|WORD_ID),
  data = COMBINED
)

  # landing zone 4 - failed
mod <- lmer(
  FIRST_LANDING ~ windowcondition * scale(log(mean_OrthoMatchModel), scale = FALSE) +
    (1 + scale(log(mean_OrthoMatchModel), scale = FALSE)|RECORDING_SESSION_LABEL) +
    (1|WORD_ID),
  data = COMBINED
)

  # landing zone 5 - successful
mod4 <- lmer(
  FIRST_LANDING ~ windowcondition * scale(log(mean_OrthoMatchModel), scale = FALSE) +
    (1|RECORDING_SESSION_LABEL) +
    (1|WORD_ID),
  data = COMBINED
)
summary(mod4)

```


