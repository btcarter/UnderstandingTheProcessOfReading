---
title: Understanding the Process of Reading
author: Benjamin T. Carter
date: 2019-10-24
output:
  html_document:
    code_folding: hide
---

## Environment

```{r message = FALSE}
library(dplyr)
library(readxl)
library(tidyr)
```

## Cleaning statistics

These are some shortcuts that can be used to view the cleaning stats via command line. I've commented them out since they only need to be run once.

```{bash}
# DIRECT=~/Box/LukeLab/UnderstandingTheProcessOfReading/data
# DATA=${DIRECT}/cleaning\ -\ UPoR1.txt # file with cleaning stats

# mv $DATA ${DIRECT}/cleaning-UPoR1.txt
# DATA=${DIRECT}/cleaning-UPoR1.txt

# tail -2 ${DATA}/cleaning-UPoR1.txt # read in last two lines
```

The stats were then taken to compute how many fix

```{r}
MERGED <- 738
DELETED <- 9985
DIR <- file.path("~","Box","LukeLab","UnderstandingTheProcessOfReading","data") # study directory
FIXS <- read.table(file.path(DIR,"FixList.xls"), header = TRUE, sep = "\t") # list of fixations
TOTAL <- as.numeric(count(FIXS)) # count total number of fixation events
REMAIN <- (MERGED + DELETED) / TOTAL
REMAIN*100 # percent of fixations that were excluded or merged.
```

## Behavior

### Data Cleaning and Aggregation

Here are the paths to the data. Let's bring them together along common variables.

```{r}
# read in the data
DIR <- file.path("~","Box","LukeLab","UnderstandingTheProcessOfReading","data") # study directory
REPORT <- read.delim(file.path(DIR,"IA_report_UPoR1.txt"), header = TRUE, sep = "\t", fill = TRUE, na.strings = ".") # interest areas report
ORTHOS <- read.csv(file.path(DIR,"Provo_Corpus-Eyetracking_Data.csv"), header = TRUE, sep = ",", fill = TRUE) # ortho probabilities

# clean the interest areas report
REPORT <- REPORT[REPORT$IA_DWELL_TIME > 0,] # remove interest areas without a fixation in them.
REPORT$RECORDING_SESSION_LABEL <- tolower(REPORT$RECORDING_SESSION_LABEL) # make label case consistent

exclude <- c("s32", "s46", "s53", "s54", "s56", "s57", "s59", "s60", "s63") # exclude messy data
REPORT <- REPORT %>% subset(!(RECORDING_SESSION_LABEL %in% exclude))


# aggregate the Provo Corpus and join to the interest areas report
COMBINED <- ORTHOS[c("Text_ID","IA_ID","OrthoMatchModel")] %>% 
  group_by(Text_ID, IA_ID) %>%
  summarize(mean_OrthoMatchModel = mean(OrthoMatchModel)) %>%
  ungroup() %>%
  right_join(REPORT,
             by = c("Text_ID" = "textnumber", "IA_ID" = "IA_ID")
             )

COMBINED <- COMBINED[!is.na(COMBINED$mean_OrthoMatchModel),] # remove na values
rm(ORTHOS, REPORT) # unload unneeded data

# need to add demographic data to this too!

# change classes
COMBINED$windowcondition <- as.factor(COMBINED$windowcondition)
COMBINED$Text_ID <- as.factor(COMBINED$Text_ID)
COMBINED$IA_ID <- as.factor(COMBINED$IA_ID)

# add variable for how far into a word someone fixated
COMBINED <- COMBINED %>%
  mutate(
    FIRST_LANDING = (IA_FIRST_FIXATION_X - IA_LEFT) / (IA_RIGHT - IA_LEFT)
  )

COMBINED$WORD_ID <- interaction(COMBINED$Text_ID, COMBINED$IA_ID, sep = ".") # create interaction variable

# check out https://link.springer.com/article/10.3758/s13414-018-1581-0
# log transform dependent variables, add additional variables from ^paper^
```

### Summary Statistics by Window Condition
```{r}
vars <- c("windowcondition", "IA_FIRST_FIXATION_DURATION", "FIRST_LANDING", "IA_DWELL_TIME")
summaryStats <- COMBINED %>%
  select(vars) %>%
  group_by(windowcondition) %>%
  summarize(
    "First Fixation Duration - mean" = mean(IA_FIRST_FIXATION_DURATION),
    "First Fixation Duration - sd" = sd(IA_FIRST_FIXATION_DURATION),
    "First Fixation Location - mean" = mean(FIRST_LANDING),
    "First Fixation Location - sd" = sd(FIRST_LANDING)) %>%
  gather(Statistic, Value, "First Fixation Duration - mean":"Landing Spot - sd") %>%
  spread(windowcondition, Value)


ggplot(COMBINED,
       aes(windowcondition, IA_FIRST_FIXATION_DURATION)) + 
  geom_jitter()

```

### Models
```{r echo = TRUE, message=FALSE, error=FALSE}
# load libraries
library(lme4)
library(lmerTest)

# models
  # first fixation duration
ortho <- lmer(
  log(IA_FIRST_FIXATION_DURATION) ~ windowcondition * scale(log(mean_OrthoMatchModel), scale = FALSE) +
    (1 + windowcondition * scale(log(mean_OrthoMatchModel), scale = FALSE)|RECORDING_SESSION_LABEL) +
    (1|WORD_ID),
  data = COMBINED
)
summary(ortho)





simple <- lmer(IA_FIRST_FIXATION_DURATION ~ windowcondition +
                 (1 | RECORDING_SESSION_LABEL),
               data = COMBINED
                 )

kable(simple, caption = "Simple Model with")

simplePlus <- lmer(
  IA_FIRST_FIXATION_DURATION ~ windowcondition +
    (1|RECORDING_SESSION_LABEL) +
    (1|Text_ID),
  data = COMBINED
  )

complex <- lmer(
  IA_FIRST_FIXATION_DURATION ~ windowcondition +
    (1 + windowcondition|RECORDING_SESSION_LABEL),
  data = COMBINED
  )

complexPlus <- lmer(
  IA_FIRST_FIXATION_DURATION ~ windowcondition +
    (1 + windowcondition|RECORDING_SESSION_LABEL) +
    (1 + windowcondition|Text_ID),
  data = COMBINED
  )

kable

  # landing zone
landing <- lmer(
  FIRST_LANDING ~ windowcondition + 
    (1 + windowcondition|RECORDING_SESSION_LABEL),
  data = COMBINED
)
  # next saccade
# nextSaccade <- lmer()
```


