---
title: "Understanding the Process of Reading"
author: "Benjamin T. Carter"
date: '2019-11-26'
output:
  word_document:
    reference_docx: style.docx
  html_document:
    df_print: paged
    code_folding: hide
---

### Environment

```{r message = FALSE}
library(dplyr)
library(readxl)
library(tidyr)
library(lme4)
library(lmerTest)
library(sjPlot)
library(ggplot2)
```

## Data Viewer Cleaning

<!-- These are some shortcuts that can be used to view the cleaning stats via command line. -->

```{bash bash_shortcuts}
# DIRECT=~/Box/LukeLab/UnderstandingTheProcessOfReading/data
# DATA=${DIRECT}/cleaning\ -\ UPoR1.txt # file with cleaning stats

# mv $DATA ${DIRECT}/cleaning-UPoR1.txt
# DATA=${DIRECT}/cleaning-UPoR1.txt

# tail -2 ${DATA}/cleaning-UPoR1.txt # read in last two lines
```

```{r dataViewer_stats, echo=FALSE, error=FALSE, message=FALSE}
MERGED <- 738
DELETED <- 9985
DIR <- file.path("~","Box","LukeLab","UnderstandingTheProcessOfReading","data") # study directory
FIXS <- read.table(file.path(DIR,"FixList.xls"), header = TRUE, sep = "\t") # list of fixations
TOTAL <- as.numeric(count(FIXS)) # count total number of fixation events
REMAIN <- (MERGED + DELETED) / TOTAL
PER <- REMAIN*100 # percent of fixations that were excluded or merged.
```

`r PER`% of the fixations were excluded or merged by Data Viewer.

## R Analysis

### Data Cleaning and Aggregation

The following steps were taken during preprocessing and cleaning of the data:

1. 9 total participants were excluded from the study due to poor eye tracking.
2. Lexical predictability values were then referenced from the Provo Corpus and inserted as a variable.
3. The variable `windowcondition` was changed from a numeric to a factor.
4. `Text_ID` was changed from a string to a factor.
5. `IA_ID` was changed from a numeric to a factor.
6. The names of the levels for `windowcondition` were changed such that 0 became "No Preview" and 1 became "Preview" for ease of comparison and graphing later.
1. A variable for the location of the first fixation was created (`FIRST_LANDING`).

####  Changes from V1
1. The number of characters in each word was entered as a separate variable (`Word_Length` from the Provo Corpus).
1. All words not fixated in first pass ('IA_SKIP' = 1) have now been removed from the analysis.

####  Changes from V2
1. Words less than 5 and greater than 9 characters have been excluded.
1. Function words have been excluded.
1. Words following a skipped word are excluded.

#### Changes from V3
1. Word_Length was centered via scale.
1. Word_Length interaction was removed.

```{r preprocessing, error=FALSE, message=FALSE, echo=FALSE}
# read in the data
DIR <- file.path("~","Box","LukeLab","UnderstandingTheProcessOfReading","data") # study directory
REPORT <- read.delim(file.path(DIR,"IA_report_UPoR1.txt"), header = TRUE, sep = "\t", fill = TRUE, na.strings = ".") # interest areas report
ORTHOS <- read.csv(file.path(DIR,"Provo_Corpus-Eyetracking_Data.csv"), header = TRUE, sep = ",", fill = TRUE) # ortho probabilities

# clean the interest areas report
REPORT$RECORDING_SESSION_LABEL <- tolower(REPORT$RECORDING_SESSION_LABEL) # make label case consistent

exclude <- c("s32", "s46", "s53", "s54", "s56", "s57", "s59", "s60", "s63") # exclude bad data
REPORT <- REPORT %>% subset(!(RECORDING_SESSION_LABEL %in% exclude))


# aggregate the Provo Corpus and join to the interest areas report
COMBINED <- ORTHOS[c("Text_ID","IA_ID","OrthoMatchModel","Word_Length","Word_Content_Or_Function")] %>% 
  filter(is.na(Word_Length) != TRUE,
         is.na(Word_Content_Or_Function) != TRUE
         ) %>%
  group_by(Text_ID, IA_ID, OrthoMatchModel, Word_Length, Word_Content_Or_Function) %>%
  summarize(mean_OrthoMatchModel = mean(OrthoMatchModel)) %>%
  ungroup() %>%
  right_join(REPORT,
             by = c("Text_ID" = "textnumber", "IA_ID" = "IA_ID")
             )

COMBINED <- COMBINED[!is.na(COMBINED$mean_OrthoMatchModel),] # remove na values
rm(ORTHOS, REPORT) # unload unneeded data

# change classes
COMBINED$windowcondition <- as.factor(COMBINED$windowcondition)
COMBINED$Text_ID <- as.factor(COMBINED$Text_ID)
COMBINED$IA_ID <- as.factor(COMBINED$IA_ID)
levels(COMBINED$windowcondition) <- list("No Preview"="0", "Preview" = "1")

# create variable for skipping previous word
COMBINED <- COMBINED %>%
  group_by(RECORDING_SESSION_LABEL) %>%
  arrange(RECORDING_SESSION_LABEL, Text_ID, IA_ID) %>%
  mutate(
    IA_SKIP_PREV = lag(IA_SKIP, n=1)
  ) %>%
  ungroup()


# add variables and filter
COMBINED <- COMBINED %>%
  mutate(
    FIRST_LANDING = (IA_FIRST_FIXATION_X - IA_LEFT) / (IA_RIGHT - IA_LEFT),
    WORD_ID = interaction(COMBINED$Text_ID, COMBINED$IA_ID, sep = ".")  # create interaction variable
  ) %>%
  filter(
    Word_Length < 9,
    Word_Length > 5, # exclude words greater than 9 characters and less than 5 characters in length
    IA_SKIP != 1, # remove words that were not skipped the first time through.
    Word_Content_Or_Function == "Content", # keep content words
    IA_SKIP_PREV == 0 # keep words that did not have the previous word skipped.
    ) %>%
  mutate(
    Word_Length = scale(Word_Length, scale = FALSE)
  )

# check out https://link.springer.com/article/10.3758/s13414-018-1581-0
# log transform dependent variables, add additional variables from ^paper^
```

### Summary Statistics by Window Condition

```{r summary_stats, error=FALSE, message=FALSE, echo=FALSE}
# Skipping, refixation and regression probability
mean_sd <- function(a) {
  b <- round(mean(a, na.rm = TRUE), digits = 2)
  c <- round(sd(a, na.rm = TRUE), digits = 2)
  return(paste(b," (",c,")",sep=""))
}

sumStats1 <- COMBINED %>%
  group_by(windowcondition, WORD_ID) %>%
  summarise(
    skipping_prob = sum(IA_SKIP)/n(),
    refixation_prob = sum(IA_FIXATION_COUNT >= 2)/n(),
    regression_prob = sum(IA_REGRESSION_IN, na.rm = TRUE)/n()
  ) %>%
  ungroup() %>%
  group_by(windowcondition) %>%
  summarise(
    "Refixation probability" = mean_sd(refixation_prob),
    "Regression probability" = mean_sd(regression_prob)
  ) %>%
  gather(Statistic, Value, "Refixation probability":"Regression probability") %>%
  spread(windowcondition, Value)

# first fixation duration, location and dwell time
sumStats2 <- COMBINED %>%
  group_by(windowcondition) %>%
  summarize(
    "First Fixation Duration" = mean_sd(IA_FIRST_FIXATION_DURATION),
    "First Fixation Location" = mean_sd(FIRST_LANDING),
    "Dwell Time" = mean_sd(IA_DWELL_TIME),
    "Gaze Duration" = mean_sd(IA_FIRST_RUN_DWELL_TIME)
    ) %>%
  gather(Statistic, Value, "First Fixation Duration":"Gaze Duration") %>%
  spread(windowcondition, Value)

# combine into single table
sumStats <- rbind(sumStats2,sumStats1)
knitr::kable(sumStats)
```

### Visualizing trends

I constructed some visualizations of the effect of window condition and text predictability on first fixation duration, gaze duration, and total dwell time. The most pronounced effects are for the first fixation.

```{r trends, error=FALSE, message=FALSE, echo=FALSE}
ggplot(COMBINED %>% filter(IA_DWELL_TIME > 0), 
       aes(x = scale(log(mean_OrthoMatchModel), scale = FALSE), y = IA_FIRST_FIXATION_DURATION, fill = windowcondition)) +
  geom_smooth(method = "lm") +
  labs(
    title = "The Effect of Preview and Frequency on First Fixation Duration ",
    x = "log Predictability",
    y = "First Fixation Duration (ms)",
    fill = "Window Condition"
  )

ggplot(COMBINED %>% filter(IA_DWELL_TIME > 0), 
       aes(x = scale(log(mean_OrthoMatchModel), scale = FALSE), y = IA_FIRST_RUN_DWELL_TIME, fill = windowcondition)) +
  geom_smooth(method = "lm") +
  labs(
    title = "The Effect of Preview and Frequency on Gaze Duration",
    x = "log Predictability",
    y = "Gaze Duration (ms)",
    fill = "Window Condition"
  )

ggplot(COMBINED %>% filter(IA_DWELL_TIME > 0), 
       aes(x = scale(log(mean_OrthoMatchModel), scale = FALSE), y = IA_DWELL_TIME, fill = windowcondition)) +
  geom_smooth(method = "lm") +
  labs(
    title = "The Effect of Preview and Frequency on Dwell Time",
    x = "log Predictability",
    y = "Total Dwell Time (ms)",
    fill = "Window Condition"
  )

```

## Models

For the models below I excluded all interest areas that did not have a fixation.

### Model for First Fixation Duration

I constructed two models for Fixation Duration. The first incorporated random slopes by window condition for each participant. The second did not.

#### Model 1:

`log(IA_FIRST_FIXATION_DURATION) ~ windowcondition * scale(log(mean_OrthoMatchModel), scale = FALSE) + (1 + windowcondition|RECORDING_SESSION_LABEL) + (1|WORD_ID)`

```{r fixationDuration_1, error=FALSE, message=FALSE, echo=FALSE}
# first fixation duration - failed to converge
# mod <- lmer(
#   log(IA_FIRST_FIXATION_DURATION) ~ windowcondition * scale(log(mean_OrthoMatchModel), scale = FALSE) +
#     (1 + windowcondition * scale(log(mean_OrthoMatchModel), scale = FALSE)|RECORDING_SESSION_LABEL) +
#     (1|WORD_ID),
#   data = COMBINED %>% filter(IA_DWELL_TIME > 0)
# )

  # second attempt - also failed to converge
# mod <- lmer(
#   log(IA_FIRST_FIXATION_DURATION) ~ windowcondition + scale(log(mean_OrthoMatchModel), scale = FALSE) +
#     (1 + windowcondition + scale(log(mean_OrthoMatchModel), scale = FALSE)|RECORDING_SESSION_LABEL) +
#     (1|WORD_ID),
#   data = COMBINED %>% filter(IA_DWELL_TIME > 0)
# )

  # third attempt - successful
mod1 <- lmer(
  log(IA_FIRST_FIXATION_DURATION) ~ windowcondition * scale(log(mean_OrthoMatchModel), scale = FALSE) +
    (1 + windowcondition|RECORDING_SESSION_LABEL) +
    (1|WORD_ID),
  data = COMBINED %>% filter(IA_DWELL_TIME > 0)
)
tab1 <- summary(mod1)
knitr::kable(tab1$coefficients[,-3], digits = 3)
```

#### Model 2:

`IA_FIRST_FIXATION_DURATION) ~ windowcondition * scale(log(mean_OrthoMatchModel), scale = FALSE) + (1|RECORDING_SESSION_LABEL) + (1|WORD_ID)`

```{r fixationDuration_2, error=FALSE, message=FALSE, echo=FALSE}
  # fourth model - failed to converge
# mod <- lmer(
#   log(IA_FIRST_FIXATION_DURATION) ~ windowcondition * scale(log(mean_OrthoMatchModel), scale = FALSE) +
#     (1 + scale(log(mean_OrthoMatchModel), scale = FALSE)|RECORDING_SESSION_LABEL) +
#     (1|WORD_ID),
#   data = COMBINED %>% filter(IA_DWELL_TIME > 0)
# )

  # fifth model - successful
mod2 <- lmer(
  log(IA_FIRST_FIXATION_DURATION) ~ windowcondition * scale(log(mean_OrthoMatchModel), scale = FALSE) +
    (1|RECORDING_SESSION_LABEL) +
    (1|WORD_ID),
  data = COMBINED %>% filter(IA_DWELL_TIME > 0)
)
tab2 <- summary(mod2)
knitr::kable(tab2$coefficients[,-3], digits = 3)
```


### Model for Gaze Duration

`log(IA_FIRST_RUN_DWELL_TIME) ~ windowcondition * scale(log(mean_OrthoMatchModel), scale = FALSE) + (1 + windowcondition|RECORDING_SESSION_LABEL) + (1|WORD_ID)`

```{r gazeDuration, error=FALSE, message=FALSE, echo=FALSE}
# gaze duration 1 - boundary (singular)
# mod <- lmer(
#    IA_FIRST_RUN_DWELL_TIME ~ windowcondition * scale(log(mean_OrthoMatchModel), scale = FALSE) +
#      (1 + windowcondition * scale(log(mean_OrthoMatchModel), scale = FALSE)|RECORDING_SESSION_LABEL) +
#      (1|WORD_ID),
#    data = COMBINED
#  )

# gaze duration 2 - failed to converge
# mod <- lmer(
#    IA_FIRST_RUN_DWELL_TIME ~ windowcondition + scale(log(mean_OrthoMatchModel), scale = FALSE) +
#      (1 + windowcondition + scale(log(mean_OrthoMatchModel), scale = FALSE)|RECORDING_SESSION_LABEL) +
#      (1|WORD_ID),
#    data = COMBINED
#  )

# gaze duration 3 - failed to converge
# mod <- lmer(
#    IA_FIRST_RUN_DWELL_TIME ~ windowcondition * scale(log(mean_OrthoMatchModel), scale = FALSE) +
#      (1 + windowcondition|RECORDING_SESSION_LABEL) +
#      (1|WORD_ID),
#    data = COMBINED %>% filter(IA_DWELL_TIME > 0)
#  )

# gaze duration 4
mod5 <- lmer(
   log(IA_FIRST_RUN_DWELL_TIME) ~ windowcondition * scale(log(mean_OrthoMatchModel), scale = FALSE) +
     (1|RECORDING_SESSION_LABEL) +
     (1|WORD_ID),
   data = COMBINED %>% filter(IA_DWELL_TIME > 0)
 )
tab5 <- summary(mod5)
knitr::kable(tab5$coefficients[,-3], digits = 3)
```


### Model for Total Dwell Time

`log(IA_DWELL_TIME) ~ windowcondition * scale(log(mean_OrthoMatchModel), scale = FALSE) + (1 + windowcondition|RECORDING_SESSION_LABEL) + (1|WORD_ID)`

```{r dwellTime, error=FALSE, message=FALSE, echo=FALSE}
 # first model - failed to converge
# mod <- lmer(
#   log(IA_DWELL_TIME) ~ windowcondition * scale(log(mean_OrthoMatchModel), scale = FALSE) +
#     (1 + windowcondition * scale(log(mean_OrthoMatchModel), scale = FALSE)|RECORDING_SESSION_LABEL) +
#     (1|WORD_ID),
#   data = COMBINED
# )

  # second model - failed
# mod <- lmer(
#   log(IA_DWELL_TIME) ~ windowcondition + scale(log(mean_OrthoMatchModel), scale = FALSE) +
#     (1 + windowcondition + scale(log(mean_OrthoMatchModel), scale = FALSE)|RECORDING_SESSION_LABEL) +
#     (1|WORD_ID),
#   data = COMBINED
# )

  # third model
mod3 <- lmer(
  log(IA_DWELL_TIME) ~ windowcondition * scale(log(mean_OrthoMatchModel), scale = FALSE) +
    (1 + windowcondition|RECORDING_SESSION_LABEL) +
    (1|WORD_ID),
  data = COMBINED %>% filter(IA_DWELL_TIME > 0)
)
tab3 <- summary(mod3)
knitr::kable(tab3$coefficients[,-3], digits = 3)
```



